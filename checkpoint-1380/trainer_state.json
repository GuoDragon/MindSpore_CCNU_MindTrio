{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1380,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021739130434782608,
      "grad_norm": 4.457962989807129,
      "learning_rate": 2.9804347826086958e-05,
      "loss": 4.5547,
      "step": 10
    },
    {
      "epoch": 0.043478260869565216,
      "grad_norm": 3.967782974243164,
      "learning_rate": 2.9586956521739132e-05,
      "loss": 3.4098,
      "step": 20
    },
    {
      "epoch": 0.06521739130434782,
      "grad_norm": 4.115760326385498,
      "learning_rate": 2.9369565217391306e-05,
      "loss": 2.5344,
      "step": 30
    },
    {
      "epoch": 0.08695652173913043,
      "grad_norm": 3.542131185531616,
      "learning_rate": 2.915217391304348e-05,
      "loss": 1.9698,
      "step": 40
    },
    {
      "epoch": 0.10869565217391304,
      "grad_norm": 2.3313612937927246,
      "learning_rate": 2.8934782608695654e-05,
      "loss": 1.7212,
      "step": 50
    },
    {
      "epoch": 0.13043478260869565,
      "grad_norm": 3.302569627761841,
      "learning_rate": 2.8717391304347828e-05,
      "loss": 1.5305,
      "step": 60
    },
    {
      "epoch": 0.15217391304347827,
      "grad_norm": 4.124090194702148,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.4155,
      "step": 70
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 2.5180137157440186,
      "learning_rate": 2.8282608695652176e-05,
      "loss": 1.3218,
      "step": 80
    },
    {
      "epoch": 0.1956521739130435,
      "grad_norm": 3.6860156059265137,
      "learning_rate": 2.806521739130435e-05,
      "loss": 1.2873,
      "step": 90
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 2.6201603412628174,
      "learning_rate": 2.784782608695652e-05,
      "loss": 1.289,
      "step": 100
    },
    {
      "epoch": 0.2391304347826087,
      "grad_norm": 2.433722496032715,
      "learning_rate": 2.7630434782608697e-05,
      "loss": 1.2278,
      "step": 110
    },
    {
      "epoch": 0.2608695652173913,
      "grad_norm": 4.565911769866943,
      "learning_rate": 2.741304347826087e-05,
      "loss": 1.2498,
      "step": 120
    },
    {
      "epoch": 0.2826086956521739,
      "grad_norm": 2.476033926010132,
      "learning_rate": 2.7195652173913045e-05,
      "loss": 1.1646,
      "step": 130
    },
    {
      "epoch": 0.30434782608695654,
      "grad_norm": 3.2127811908721924,
      "learning_rate": 2.6978260869565216e-05,
      "loss": 1.1994,
      "step": 140
    },
    {
      "epoch": 0.32608695652173914,
      "grad_norm": 2.410806894302368,
      "learning_rate": 2.6760869565217393e-05,
      "loss": 1.1331,
      "step": 150
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 2.748440742492676,
      "learning_rate": 2.6543478260869567e-05,
      "loss": 1.0391,
      "step": 160
    },
    {
      "epoch": 0.3695652173913043,
      "grad_norm": 3.201950788497925,
      "learning_rate": 2.6326086956521738e-05,
      "loss": 1.0559,
      "step": 170
    },
    {
      "epoch": 0.391304347826087,
      "grad_norm": 2.4877686500549316,
      "learning_rate": 2.6108695652173915e-05,
      "loss": 1.0402,
      "step": 180
    },
    {
      "epoch": 0.41304347826086957,
      "grad_norm": 3.674781322479248,
      "learning_rate": 2.589130434782609e-05,
      "loss": 0.9925,
      "step": 190
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 2.9895176887512207,
      "learning_rate": 2.567391304347826e-05,
      "loss": 1.0627,
      "step": 200
    },
    {
      "epoch": 0.45652173913043476,
      "grad_norm": 2.8221993446350098,
      "learning_rate": 2.5456521739130433e-05,
      "loss": 1.0613,
      "step": 210
    },
    {
      "epoch": 0.4782608695652174,
      "grad_norm": 2.267896890640259,
      "learning_rate": 2.523913043478261e-05,
      "loss": 1.0154,
      "step": 220
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.1297924518585205,
      "learning_rate": 2.5021739130434784e-05,
      "loss": 1.1127,
      "step": 230
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 2.805244207382202,
      "learning_rate": 2.4804347826086955e-05,
      "loss": 0.966,
      "step": 240
    },
    {
      "epoch": 0.5434782608695652,
      "grad_norm": 3.4790074825286865,
      "learning_rate": 2.4586956521739132e-05,
      "loss": 1.0534,
      "step": 250
    },
    {
      "epoch": 0.5652173913043478,
      "grad_norm": 2.655440330505371,
      "learning_rate": 2.4369565217391306e-05,
      "loss": 0.9778,
      "step": 260
    },
    {
      "epoch": 0.5869565217391305,
      "grad_norm": 2.7947003841400146,
      "learning_rate": 2.4152173913043477e-05,
      "loss": 1.0029,
      "step": 270
    },
    {
      "epoch": 0.6086956521739131,
      "grad_norm": 2.6129636764526367,
      "learning_rate": 2.393478260869565e-05,
      "loss": 0.9411,
      "step": 280
    },
    {
      "epoch": 0.6304347826086957,
      "grad_norm": 3.5274460315704346,
      "learning_rate": 2.3717391304347828e-05,
      "loss": 0.9844,
      "step": 290
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 3.444638729095459,
      "learning_rate": 2.3500000000000002e-05,
      "loss": 1.0313,
      "step": 300
    },
    {
      "epoch": 0.6739130434782609,
      "grad_norm": 2.9339098930358887,
      "learning_rate": 2.3282608695652173e-05,
      "loss": 0.9008,
      "step": 310
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 3.249237298965454,
      "learning_rate": 2.306521739130435e-05,
      "loss": 0.8924,
      "step": 320
    },
    {
      "epoch": 0.717391304347826,
      "grad_norm": 2.9385483264923096,
      "learning_rate": 2.2847826086956524e-05,
      "loss": 0.9976,
      "step": 330
    },
    {
      "epoch": 0.7391304347826086,
      "grad_norm": 2.493161916732788,
      "learning_rate": 2.2630434782608694e-05,
      "loss": 0.9521,
      "step": 340
    },
    {
      "epoch": 0.7608695652173914,
      "grad_norm": 3.222437858581543,
      "learning_rate": 2.241304347826087e-05,
      "loss": 0.8793,
      "step": 350
    },
    {
      "epoch": 0.782608695652174,
      "grad_norm": 2.512014150619507,
      "learning_rate": 2.2195652173913046e-05,
      "loss": 0.9551,
      "step": 360
    },
    {
      "epoch": 0.8043478260869565,
      "grad_norm": 3.0467529296875,
      "learning_rate": 2.1978260869565216e-05,
      "loss": 1.0069,
      "step": 370
    },
    {
      "epoch": 0.8260869565217391,
      "grad_norm": 3.1954846382141113,
      "learning_rate": 2.176086956521739e-05,
      "loss": 0.838,
      "step": 380
    },
    {
      "epoch": 0.8478260869565217,
      "grad_norm": 3.138439178466797,
      "learning_rate": 2.1543478260869567e-05,
      "loss": 0.96,
      "step": 390
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 2.294872999191284,
      "learning_rate": 2.132608695652174e-05,
      "loss": 0.9074,
      "step": 400
    },
    {
      "epoch": 0.8913043478260869,
      "grad_norm": 3.6454548835754395,
      "learning_rate": 2.1108695652173912e-05,
      "loss": 0.9624,
      "step": 410
    },
    {
      "epoch": 0.9130434782608695,
      "grad_norm": 2.7551369667053223,
      "learning_rate": 2.089130434782609e-05,
      "loss": 0.9157,
      "step": 420
    },
    {
      "epoch": 0.9347826086956522,
      "grad_norm": 2.485288619995117,
      "learning_rate": 2.0673913043478263e-05,
      "loss": 0.8874,
      "step": 430
    },
    {
      "epoch": 0.9565217391304348,
      "grad_norm": 2.5314090251922607,
      "learning_rate": 2.0456521739130434e-05,
      "loss": 0.8958,
      "step": 440
    },
    {
      "epoch": 0.9782608695652174,
      "grad_norm": 2.392038583755493,
      "learning_rate": 2.0239130434782608e-05,
      "loss": 0.8209,
      "step": 450
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.6270816326141357,
      "learning_rate": 2.0021739130434785e-05,
      "loss": 0.9047,
      "step": 460
    },
    {
      "epoch": 1.0217391304347827,
      "grad_norm": 2.8138768672943115,
      "learning_rate": 1.9804347826086955e-05,
      "loss": 0.8738,
      "step": 470
    },
    {
      "epoch": 1.0434782608695652,
      "grad_norm": 3.539304733276367,
      "learning_rate": 1.958695652173913e-05,
      "loss": 0.8859,
      "step": 480
    },
    {
      "epoch": 1.065217391304348,
      "grad_norm": 2.4639077186584473,
      "learning_rate": 1.9369565217391307e-05,
      "loss": 0.8573,
      "step": 490
    },
    {
      "epoch": 1.0869565217391304,
      "grad_norm": 4.146035194396973,
      "learning_rate": 1.915217391304348e-05,
      "loss": 0.7793,
      "step": 500
    },
    {
      "epoch": 1.108695652173913,
      "grad_norm": 2.6396822929382324,
      "learning_rate": 1.893478260869565e-05,
      "loss": 0.7942,
      "step": 510
    },
    {
      "epoch": 1.1304347826086956,
      "grad_norm": 2.898421049118042,
      "learning_rate": 1.8717391304347825e-05,
      "loss": 0.8504,
      "step": 520
    },
    {
      "epoch": 1.1521739130434783,
      "grad_norm": 2.9132394790649414,
      "learning_rate": 1.8500000000000002e-05,
      "loss": 0.8871,
      "step": 530
    },
    {
      "epoch": 1.1739130434782608,
      "grad_norm": 3.0265746116638184,
      "learning_rate": 1.8282608695652173e-05,
      "loss": 0.8126,
      "step": 540
    },
    {
      "epoch": 1.1956521739130435,
      "grad_norm": 2.218928337097168,
      "learning_rate": 1.8065217391304347e-05,
      "loss": 0.9189,
      "step": 550
    },
    {
      "epoch": 1.2173913043478262,
      "grad_norm": 3.5533411502838135,
      "learning_rate": 1.7847826086956524e-05,
      "loss": 0.847,
      "step": 560
    },
    {
      "epoch": 1.2391304347826086,
      "grad_norm": 3.312528133392334,
      "learning_rate": 1.7630434782608695e-05,
      "loss": 0.8084,
      "step": 570
    },
    {
      "epoch": 1.2608695652173914,
      "grad_norm": 3.2278223037719727,
      "learning_rate": 1.741304347826087e-05,
      "loss": 0.8264,
      "step": 580
    },
    {
      "epoch": 1.2826086956521738,
      "grad_norm": 3.1146812438964844,
      "learning_rate": 1.7195652173913043e-05,
      "loss": 0.8339,
      "step": 590
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 2.9311509132385254,
      "learning_rate": 1.697826086956522e-05,
      "loss": 0.7379,
      "step": 600
    },
    {
      "epoch": 1.3260869565217392,
      "grad_norm": 3.0476245880126953,
      "learning_rate": 1.676086956521739e-05,
      "loss": 0.7606,
      "step": 610
    },
    {
      "epoch": 1.3478260869565217,
      "grad_norm": 2.56320858001709,
      "learning_rate": 1.6543478260869564e-05,
      "loss": 0.8877,
      "step": 620
    },
    {
      "epoch": 1.3695652173913042,
      "grad_norm": 3.0849907398223877,
      "learning_rate": 1.6326086956521742e-05,
      "loss": 0.8429,
      "step": 630
    },
    {
      "epoch": 1.391304347826087,
      "grad_norm": 2.3039817810058594,
      "learning_rate": 1.6108695652173912e-05,
      "loss": 0.7907,
      "step": 640
    },
    {
      "epoch": 1.4130434782608696,
      "grad_norm": 2.9477241039276123,
      "learning_rate": 1.5891304347826086e-05,
      "loss": 0.8219,
      "step": 650
    },
    {
      "epoch": 1.434782608695652,
      "grad_norm": 4.673061370849609,
      "learning_rate": 1.5673913043478264e-05,
      "loss": 0.8513,
      "step": 660
    },
    {
      "epoch": 1.4565217391304348,
      "grad_norm": 2.629234790802002,
      "learning_rate": 1.5456521739130437e-05,
      "loss": 0.6547,
      "step": 670
    },
    {
      "epoch": 1.4782608695652173,
      "grad_norm": 2.3868377208709717,
      "learning_rate": 1.5239130434782608e-05,
      "loss": 0.7438,
      "step": 680
    },
    {
      "epoch": 1.5,
      "grad_norm": 2.8667562007904053,
      "learning_rate": 1.5021739130434782e-05,
      "loss": 0.7561,
      "step": 690
    },
    {
      "epoch": 1.5217391304347827,
      "grad_norm": 4.453482151031494,
      "learning_rate": 1.4804347826086956e-05,
      "loss": 0.7393,
      "step": 700
    },
    {
      "epoch": 1.5434782608695652,
      "grad_norm": 2.645048141479492,
      "learning_rate": 1.4586956521739132e-05,
      "loss": 0.7071,
      "step": 710
    },
    {
      "epoch": 1.5652173913043477,
      "grad_norm": 3.699719190597534,
      "learning_rate": 1.4369565217391304e-05,
      "loss": 0.7829,
      "step": 720
    },
    {
      "epoch": 1.5869565217391304,
      "grad_norm": 3.0587117671966553,
      "learning_rate": 1.4152173913043478e-05,
      "loss": 0.7965,
      "step": 730
    },
    {
      "epoch": 1.608695652173913,
      "grad_norm": 2.8147547245025635,
      "learning_rate": 1.3934782608695653e-05,
      "loss": 0.7039,
      "step": 740
    },
    {
      "epoch": 1.6304347826086958,
      "grad_norm": 2.822070837020874,
      "learning_rate": 1.3717391304347826e-05,
      "loss": 0.8229,
      "step": 750
    },
    {
      "epoch": 1.6521739130434783,
      "grad_norm": 2.7264626026153564,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.7607,
      "step": 760
    },
    {
      "epoch": 1.6739130434782608,
      "grad_norm": 2.533402919769287,
      "learning_rate": 1.3282608695652173e-05,
      "loss": 0.7009,
      "step": 770
    },
    {
      "epoch": 1.6956521739130435,
      "grad_norm": 2.467053174972534,
      "learning_rate": 1.3065217391304349e-05,
      "loss": 0.81,
      "step": 780
    },
    {
      "epoch": 1.7173913043478262,
      "grad_norm": 2.652350902557373,
      "learning_rate": 1.2847826086956523e-05,
      "loss": 0.7764,
      "step": 790
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 2.696833372116089,
      "learning_rate": 1.2630434782608695e-05,
      "loss": 0.7609,
      "step": 800
    },
    {
      "epoch": 1.7608695652173914,
      "grad_norm": 4.315341949462891,
      "learning_rate": 1.2413043478260871e-05,
      "loss": 0.6884,
      "step": 810
    },
    {
      "epoch": 1.7826086956521738,
      "grad_norm": 3.717758893966675,
      "learning_rate": 1.2195652173913043e-05,
      "loss": 0.7264,
      "step": 820
    },
    {
      "epoch": 1.8043478260869565,
      "grad_norm": 3.238192081451416,
      "learning_rate": 1.1978260869565219e-05,
      "loss": 0.7357,
      "step": 830
    },
    {
      "epoch": 1.8260869565217392,
      "grad_norm": 3.218017339706421,
      "learning_rate": 1.1760869565217391e-05,
      "loss": 0.781,
      "step": 840
    },
    {
      "epoch": 1.8478260869565217,
      "grad_norm": 4.146176815032959,
      "learning_rate": 1.1543478260869565e-05,
      "loss": 0.7548,
      "step": 850
    },
    {
      "epoch": 1.8695652173913042,
      "grad_norm": 3.0595662593841553,
      "learning_rate": 1.132608695652174e-05,
      "loss": 0.734,
      "step": 860
    },
    {
      "epoch": 1.891304347826087,
      "grad_norm": 2.728989839553833,
      "learning_rate": 1.1108695652173913e-05,
      "loss": 0.739,
      "step": 870
    },
    {
      "epoch": 1.9130434782608696,
      "grad_norm": 4.526447296142578,
      "learning_rate": 1.0891304347826088e-05,
      "loss": 0.7955,
      "step": 880
    },
    {
      "epoch": 1.9347826086956523,
      "grad_norm": 3.037853717803955,
      "learning_rate": 1.067391304347826e-05,
      "loss": 0.7744,
      "step": 890
    },
    {
      "epoch": 1.9565217391304348,
      "grad_norm": 3.5429975986480713,
      "learning_rate": 1.0456521739130435e-05,
      "loss": 0.7645,
      "step": 900
    },
    {
      "epoch": 1.9782608695652173,
      "grad_norm": 3.2651281356811523,
      "learning_rate": 1.0239130434782608e-05,
      "loss": 0.7734,
      "step": 910
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.275511264801025,
      "learning_rate": 1.0021739130434782e-05,
      "loss": 0.677,
      "step": 920
    },
    {
      "epoch": 2.0217391304347827,
      "grad_norm": 3.377823829650879,
      "learning_rate": 9.804347826086958e-06,
      "loss": 0.7902,
      "step": 930
    },
    {
      "epoch": 2.0434782608695654,
      "grad_norm": 3.166989326477051,
      "learning_rate": 9.58695652173913e-06,
      "loss": 0.7619,
      "step": 940
    },
    {
      "epoch": 2.0652173913043477,
      "grad_norm": 3.8157553672790527,
      "learning_rate": 9.369565217391304e-06,
      "loss": 0.7767,
      "step": 950
    },
    {
      "epoch": 2.0869565217391304,
      "grad_norm": 3.1043598651885986,
      "learning_rate": 9.152173913043478e-06,
      "loss": 0.7075,
      "step": 960
    },
    {
      "epoch": 2.108695652173913,
      "grad_norm": 3.150958299636841,
      "learning_rate": 8.934782608695652e-06,
      "loss": 0.6588,
      "step": 970
    },
    {
      "epoch": 2.130434782608696,
      "grad_norm": 3.377756118774414,
      "learning_rate": 8.717391304347828e-06,
      "loss": 0.6462,
      "step": 980
    },
    {
      "epoch": 2.1521739130434785,
      "grad_norm": 3.5145504474639893,
      "learning_rate": 8.5e-06,
      "loss": 0.7598,
      "step": 990
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 3.383389472961426,
      "learning_rate": 8.282608695652174e-06,
      "loss": 0.6912,
      "step": 1000
    },
    {
      "epoch": 2.1956521739130435,
      "grad_norm": 3.149501085281372,
      "learning_rate": 8.065217391304348e-06,
      "loss": 0.6827,
      "step": 1010
    },
    {
      "epoch": 2.217391304347826,
      "grad_norm": 3.3387067317962646,
      "learning_rate": 7.847826086956522e-06,
      "loss": 0.6576,
      "step": 1020
    },
    {
      "epoch": 2.239130434782609,
      "grad_norm": 3.2685811519622803,
      "learning_rate": 7.630434782608696e-06,
      "loss": 0.6979,
      "step": 1030
    },
    {
      "epoch": 2.260869565217391,
      "grad_norm": 3.488847255706787,
      "learning_rate": 7.41304347826087e-06,
      "loss": 0.6793,
      "step": 1040
    },
    {
      "epoch": 2.282608695652174,
      "grad_norm": 3.1964223384857178,
      "learning_rate": 7.1956521739130435e-06,
      "loss": 0.7134,
      "step": 1050
    },
    {
      "epoch": 2.3043478260869565,
      "grad_norm": 3.182490110397339,
      "learning_rate": 6.9782608695652175e-06,
      "loss": 0.6939,
      "step": 1060
    },
    {
      "epoch": 2.3260869565217392,
      "grad_norm": 4.19981050491333,
      "learning_rate": 6.760869565217392e-06,
      "loss": 0.6117,
      "step": 1070
    },
    {
      "epoch": 2.3478260869565215,
      "grad_norm": 3.291147232055664,
      "learning_rate": 6.543478260869565e-06,
      "loss": 0.6951,
      "step": 1080
    },
    {
      "epoch": 2.369565217391304,
      "grad_norm": 3.505317449569702,
      "learning_rate": 6.326086956521739e-06,
      "loss": 0.6856,
      "step": 1090
    },
    {
      "epoch": 2.391304347826087,
      "grad_norm": 3.848313570022583,
      "learning_rate": 6.108695652173913e-06,
      "loss": 0.6733,
      "step": 1100
    },
    {
      "epoch": 2.4130434782608696,
      "grad_norm": 3.205251455307007,
      "learning_rate": 5.891304347826087e-06,
      "loss": 0.7476,
      "step": 1110
    },
    {
      "epoch": 2.4347826086956523,
      "grad_norm": 3.1148452758789062,
      "learning_rate": 5.673913043478261e-06,
      "loss": 0.6555,
      "step": 1120
    },
    {
      "epoch": 2.4565217391304346,
      "grad_norm": 3.3104429244995117,
      "learning_rate": 5.456521739130434e-06,
      "loss": 0.7155,
      "step": 1130
    },
    {
      "epoch": 2.4782608695652173,
      "grad_norm": 3.5106589794158936,
      "learning_rate": 5.239130434782609e-06,
      "loss": 0.7093,
      "step": 1140
    },
    {
      "epoch": 2.5,
      "grad_norm": 3.8282525539398193,
      "learning_rate": 5.021739130434783e-06,
      "loss": 0.661,
      "step": 1150
    },
    {
      "epoch": 2.5217391304347827,
      "grad_norm": 3.2698514461517334,
      "learning_rate": 4.804347826086957e-06,
      "loss": 0.6672,
      "step": 1160
    },
    {
      "epoch": 2.5434782608695654,
      "grad_norm": 2.809046745300293,
      "learning_rate": 4.586956521739131e-06,
      "loss": 0.6357,
      "step": 1170
    },
    {
      "epoch": 2.5652173913043477,
      "grad_norm": 3.490058660507202,
      "learning_rate": 4.369565217391304e-06,
      "loss": 0.6851,
      "step": 1180
    },
    {
      "epoch": 2.5869565217391304,
      "grad_norm": 4.547445297241211,
      "learning_rate": 4.152173913043478e-06,
      "loss": 0.6619,
      "step": 1190
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 4.143148422241211,
      "learning_rate": 3.9347826086956525e-06,
      "loss": 0.7341,
      "step": 1200
    },
    {
      "epoch": 2.630434782608696,
      "grad_norm": 3.0308525562286377,
      "learning_rate": 3.7173913043478264e-06,
      "loss": 0.6961,
      "step": 1210
    },
    {
      "epoch": 2.6521739130434785,
      "grad_norm": 3.839383602142334,
      "learning_rate": 3.5e-06,
      "loss": 0.661,
      "step": 1220
    },
    {
      "epoch": 2.6739130434782608,
      "grad_norm": 3.2739200592041016,
      "learning_rate": 3.282608695652174e-06,
      "loss": 0.678,
      "step": 1230
    },
    {
      "epoch": 2.6956521739130435,
      "grad_norm": 2.8345448970794678,
      "learning_rate": 3.0652173913043482e-06,
      "loss": 0.6978,
      "step": 1240
    },
    {
      "epoch": 2.717391304347826,
      "grad_norm": 3.642965793609619,
      "learning_rate": 2.8478260869565217e-06,
      "loss": 0.694,
      "step": 1250
    },
    {
      "epoch": 2.7391304347826084,
      "grad_norm": 4.118788242340088,
      "learning_rate": 2.6304347826086957e-06,
      "loss": 0.7116,
      "step": 1260
    },
    {
      "epoch": 2.7608695652173916,
      "grad_norm": 3.1766538619995117,
      "learning_rate": 2.4130434782608696e-06,
      "loss": 0.6442,
      "step": 1270
    },
    {
      "epoch": 2.782608695652174,
      "grad_norm": 3.5130465030670166,
      "learning_rate": 2.1956521739130435e-06,
      "loss": 0.6623,
      "step": 1280
    },
    {
      "epoch": 2.8043478260869565,
      "grad_norm": 3.9755983352661133,
      "learning_rate": 1.9782608695652175e-06,
      "loss": 0.6522,
      "step": 1290
    },
    {
      "epoch": 2.8260869565217392,
      "grad_norm": 3.7048192024230957,
      "learning_rate": 1.7608695652173914e-06,
      "loss": 0.6571,
      "step": 1300
    },
    {
      "epoch": 2.8478260869565215,
      "grad_norm": 2.982729434967041,
      "learning_rate": 1.5434782608695651e-06,
      "loss": 0.6831,
      "step": 1310
    },
    {
      "epoch": 2.869565217391304,
      "grad_norm": 2.949382781982422,
      "learning_rate": 1.326086956521739e-06,
      "loss": 0.7008,
      "step": 1320
    },
    {
      "epoch": 2.891304347826087,
      "grad_norm": 3.081045150756836,
      "learning_rate": 1.1086956521739132e-06,
      "loss": 0.5647,
      "step": 1330
    },
    {
      "epoch": 2.9130434782608696,
      "grad_norm": 4.165475368499756,
      "learning_rate": 8.91304347826087e-07,
      "loss": 0.6792,
      "step": 1340
    },
    {
      "epoch": 2.9347826086956523,
      "grad_norm": 3.131399154663086,
      "learning_rate": 6.739130434782609e-07,
      "loss": 0.6714,
      "step": 1350
    },
    {
      "epoch": 2.9565217391304346,
      "grad_norm": 3.4838480949401855,
      "learning_rate": 4.5652173913043484e-07,
      "loss": 0.6068,
      "step": 1360
    },
    {
      "epoch": 2.9782608695652173,
      "grad_norm": 3.163945198059082,
      "learning_rate": 2.391304347826087e-07,
      "loss": 0.6559,
      "step": 1370
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.097847938537598,
      "learning_rate": 2.173913043478261e-08,
      "loss": 0.6512,
      "step": 1380
    }
  ],
  "logging_steps": 10,
  "max_steps": 1380,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.8579728914049024e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
