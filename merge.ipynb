{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecebb1-e6f9-4600-a481-b9e6ec334e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDTB 句间关系分类 LoRA 微调实验\n",
    "本实验使用 `MindSpore + mindnlp + LoRA` 在 `DeepSeek-R1-Distill-Qwen-1.5B` 模型上进行微调，任务目标是：\n",
    "- 输入：一个句子（或对话内容）\n",
    "- 输出：该句子属于哪一种 PDTB 分类，并解释原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c44dfa-d302-4711-a822-9df3d84c58ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/python3.10.14/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'not support import any ops for now.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.\n"
     ]
    }
   ],
   "source": [
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04bef02-4500-4438-9cfd-5c33e5ee6509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a035c183-f1e7-4f33-809d-e5ff933ed3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
    "    ms_dtype=mindspore.bfloat16,\n",
    "    device_map=0\n",
    ")\n",
    "\n",
    "model.enable_input_require_grads()  # 开启梯度检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b15c02-84ab-4653-814f-86a415ec6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练好的 checkpoint 路径\n",
    "lora_path = \"/home/ma-user/work/output/checkpoint-1380\"\n",
    "\n",
    "# 保存合并后模型的目录\n",
    "merged_path = \"/home/ma-user/work/merged_qwen_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413c7565-6c53-4394-b9e6-47ed49c9ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 LoRA 权重\n",
    "model = PeftModel.from_pretrained(model, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3170ae51-d91b-445a-9967-3624e7052151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并权重\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b5bd1a-9a74-4480-ad8d-6ed7b9cf2a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 权重已合并完成，模型保存在: /home/ma-user/work/merged_qwen_lora\n"
     ]
    }
   ],
   "source": [
    "# 保存最终模型\n",
    "model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(f\"LoRA 权重已合并完成，模型保存在: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4a947b-580c-4502-a053-cbb3149621e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗯，用户给了一个句子，让我分析句间关系。句子是：“月亮又圆又亮，所以古人称之为玉盘。”首先，我需要理解这句话的结构和逻辑关系。句子的结构是并列句式，前后两部分分别描述了月亮的圆亮和古人对其的称呼，然后通过“所以”引出结论，说明古人对月亮的称呼是“玉盘”。\n",
      "\n",
      "接下来，我要分析句间关系的类型。这里使用了因果关系，因为前半部分描述了月亮的特征，后半部分通过“所以”说明了古人对其的称呼，两者之间存在因果关系。此外，还涉及到因果关系的扩展，因为古人对月亮的称呼也反映了他们对月亮的敬仰和赞美。\n",
      "\n",
      "然后，我需要考虑句子的语义功能。这句话通过因果关系和扩展语义功能，强调了月亮的圆亮和古人对其的称呼之间的联系，从而增强了文章的连贯性和逻辑性。\n",
      "\n",
      "最后，总结句间关系的类型是因果关系和扩展语义功能，这有助于更好地理解句子的结构和逻辑关系，从而在写作时能够更好地运用这些关系来增强文章的表达效果。\n",
      "</think>\n",
      "\n",
      "句间关系：因果关系和扩展语义功能\n",
      "\n",
      "原因：前半句话描述了月亮的圆亮特征，后半句话通过“所以”引出古人对其的称呼，形成了因果关系。此外，后半句话还扩展了前半句话的语义功能，强调了月亮的圆亮与古人对其的称呼之间的联系，从而增强了文章的连贯性和逻辑性。\n"
     ]
    }
   ],
   "source": [
    "# host to device\n",
    "model = model.npu()\n",
    "\n",
    "prompt = \"月亮又圆又亮，所以古人称之为玉盘。\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"你是一个中文篇章句间关系分析师\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"ms\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad1099-1647-4631-99c8-cd271610fd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
